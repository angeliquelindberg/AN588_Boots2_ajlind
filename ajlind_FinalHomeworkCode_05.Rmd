---
title: "ajlind_OriginalHomeworkCode_05"
author: "Angelique J. Lindberg"
date: "`r Sys.Date()`"
output: 
  html_document:
    theme: cosmo
    toc: true
    toc_float: true  
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r packages needed, include=FALSE}
library(curl) #load or download these packages
library(boot) 
```


# Homework 5: Bootstrapping Standard Errors and CIs for Linear Models 

## Part 1
[1] Using the “KamilarAndCooperData.csv” dataset, run a linear regression looking at log(HomeRange_km2) in relation to log(Body_mass_female_mean) and report your β coeffiecients (slope and intercept).

Loading in Data:
```{r}
f <- curl("https://raw.githubusercontent.com/fuzzyatelin/fuzzyatelin.github.io/master/AN588_Fall23/KamilarAndCooperData.csv")
d <- read.csv(f, header = TRUE, sep = ",", stringsAsFactors =)
```

Running linear regression:
```{r}
m<-lm(data=d, log(d$HomeRange_km2)~log(d$Body_mass_female_mean))
summary(m)
```
The intercept is seen at (intercept) row: -9.44. This means that the predicted value of the log of the home range is -9.44 when log of female mean body mass is 0. The slope is seen where the y variable is (the function goes y~x), so 1.04. This means that the log of the home range is predicted to change by about 1.04 per unit of change in the log of body mass female mean.

## Part 2
Then, use bootstrapping to sample from your data 1000 times with replacement, each time fitting the same model and calculating the same coefficients. This generates a sampling distribution for each β coefficient.

Going to use bootstrapping method from R in Action p6. 306
```{r}
bs<-function(formula, data, indices){ #specifying which statistics to bootstrap
  d2<-data[indices,] #telling it to sample
  fit<-lm(formula, data=d2) #fitting the model
  return(coef(fit))#telling it to return the coefficients 
}
set.seed(923) #setting seed to be replicable
bootdist<-boot(data=d, statistic=bs, R=1000, formula=log(HomeRange_km2)~log(Body_mass_female_mean))#1000 repetitions, telling it to use the bootstrap statistic I just created, also using formula of y~x, like regular regression 
print(bootdist) #printing the results
```

Side note: this method still allows for the printing of the data, it just requires another function (I set eval to false for the sake of space and not making you scroll through 1,000 samples, but it works)
```{r, eval=FALSE}
boot.array(bootdist, indices = TRUE)
```


Estimate the standard error for each of your β coefficients as the standard deviation of the sampling distribution from your bootstrap and determine the 95% CI for each of your β coefficients based on the appropriate quantiles from your sampling distribution.

To do confidence intervals, need to use different code:
```{r}
boot.ci(bootdist, type="bca", index=1) #telling it to give me confidence interval of first index (intercept)
```

```{r}
boot.ci(bootdist, type="bca", index=2) #telling it to give me confidence interval of second index (slope)
```


How does the former compare to the SE estimated from your entire dataset using the formula for standard error implemented in lm()?

The standard error of the intercept earlier was 0.67, so that's close-ish to 0.57 (t1* line). The standard error of the slop earlier was 0.08 so that's close to 0.07.

How does the latter compare to the 95% CI estimated from your entire dataset?
```{r}
confint(m, level=0.95)
```

The 95% confidence interval  for the intercept from the bootstrap was (-10.56, -8.28) which is very close to the (-10.77, -8.11) interval from the entire data. The slope CI from bootstrap was (0.89, 1.19) which very close to the (0.87, 1.2) CI for the entire data.


## Reflection
I enjoyed this assignment and peer commentary more than any of the previous ones. I felt the most comfortable with this material and the hard part (bootstrapping) was relatively easy to figure out. I enjoyed providing commentary and suggesting some annotating and formatting edits, although both their code and my code seem to not need much editing. I still appreciated the reassurance and the comments which caused me to think about my code and results a little differently.
